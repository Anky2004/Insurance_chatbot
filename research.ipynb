{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb7c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_together import ChatTogether\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "INDEX_NAME = \"policy-index\"\n",
    "PINECONE_REGION = \"us-east-1\"\n",
    "DATA_DIR = \"data/\"  # your folder with PDFs\n",
    "\n",
    "# Load and split PDFs\n",
    "loader = DirectoryLoader(DATA_DIR, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)\n",
    "texts = splitter.split_documents(docs)\n",
    "\n",
    "# Pinecone setup\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "if INDEX_NAME not in [i.name for i in pc.list_indexes()]:\n",
    "    pc.create_index(name=INDEX_NAME, dimension=384, metric=\"cosine\",\n",
    "                    spec=ServerlessSpec(cloud=\"aws\", region=PINECONE_REGION))\n",
    "    PineconeVectorStore.from_documents(texts, embedding=embedding, index_name=INDEX_NAME)\n",
    "\n",
    "vectorstore = PineconeVectorStore.from_existing_index(index_name=INDEX_NAME, embedding=embedding)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Load Together AI model\n",
    "llm = ChatTogether(\n",
    "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    api_key=TOGETHER_API_KEY,\n",
    "    temperature=0.3,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "# Prompt templates\n",
    "parse_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Extract key details from the following query and return JSON:\n",
    "- Age\n",
    "- Gender\n",
    "- Procedure\n",
    "- Location\n",
    "- Policy Duration\n",
    "\n",
    "Query: {input}\n",
    "\"\"\")\n",
    "\n",
    "decision_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a policy decision assistant.\n",
    "\n",
    "Based on the extracted details:\n",
    "{parsed}\n",
    "\n",
    "And the following policy clauses:\n",
    "{context}\n",
    "\n",
    "Determine:\n",
    "- decision: approve/reject\n",
    "- amount: (in â‚¹)\n",
    "- justification: cite relevant clause(s)\n",
    "\n",
    "Return JSON with these keys.\n",
    "\"\"\")\n",
    "\n",
    "parser_chain = create_stuff_documents_chain(llm, parse_prompt)\n",
    "qa_chain = create_stuff_documents_chain(llm, decision_prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, qa_chain)\n",
    "\n",
    "# Sample input\n",
    "query = \"46-year-old male, knee surgery in Pune, 3-month-old insurance policy\"\n",
    "\n",
    "parsed_result = parser_chain.invoke({\"input\": query})\n",
    "print(\"Parsed:\", parsed_result)\n",
    "\n",
    "final_result = rag_chain.invoke({\"input\": str(parsed_result)})\n",
    "print(\"Decision:\", final_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
